---
title: "TDAvec vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{TDAvec-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The `TDAvec` package provides implementations of several vector summaries of persistence diagrams studied in Topological Data Analysis (TDA). Each is obtained by discretizing the associated summary function computed from a persistence diagram. The summary functions included in this package are  

1. _Persistence landscape function_
2. _Persistence silhouette function_
3. _Persistent entropy summary function_
4. _Euler characteristic curve_
5. _Betti curve_ 
6. _Persistence surface_
7. _Persistence block_

For improved computational efficiency, all code behind the vector summaries is written in `C++` using the `Rcpp` package. Whenever applicable, when compare our code with existing implementations in terms of accuracy and run-time cost. In this vignette, we illustrate the basic usage of the `TDAvec` package using simple examples.   

Let's first load the required libraries. 

```{r setup}
library(TDAvec)
library(TDA) # to compute persistence diagrams
library(microbenchmark) # to compare computational costs
```

Now, we generate a 2D point cloud of size 100 sampled uniformly from a unit circle with added Gaussian noise:

```{r}
N <- 100 # point cloud size
set.seed(123)
X <- circleUnif(N) + rnorm(2*N,mean = 0,sd = 0.2)
# plot the point cloud
plot(X,pch = 20,asp = 1)
```

Next, we use the `TDA` package to compute the persistence diagram (PD) using the Vietoris-Rips filtration built on top of the point cloud $X$.

```{r}
D <- ripsDiag(X,maxdimension = 1,maxscale = 2)$diagram
sum(D[,1]==0) # number of connected components
sum(D[,1]==1) # number of loops
sum(D[,1]==2) # number of voids
```

In the `ripsDiag()` function, `maxdimension` is the maximum homological dimension of the topological features to be computed (connected components if maxdimension=0, connected components and loops if 1, connected components, loops and voids if 2, etc.)  `maxscale` is the maximum value of the scale parameter of the filtration (which we set equal to 2 since the points are sampled from a circle with diameter 2). 

The persistence diagram $D$ has 100 connected components (the same as the point cloud size), 13 loops (one with long life-span, the rest are short-lived) and 0 voids along with their `birth` and `death` values. To plot the diagram, we can use the `plot()` function. 

```{r}
plot(D)
# the solid dots represent connected components
# the red triangles represent loops
```

Let's compute the _persistence landscape_ (PL) and _persistence silhouette_ (PS) summaries from the persistence diagram $D$.

```{r}
# sequence of scale values to vectorize the summary function
scaleSeq = seq(0,2,length.out=11) 
# compute the PL and PS summaries for homological dimension H_0
computePL(D,homDim = 0,scaleSeq,k=1)
computePS(D,homDim = 0,scaleSeq,p=1)
```

Here, the vectorization is performed by evaluating the respective summary function at each element of `scaleSeq` (i.e. $0.0,0.2,0.4,\ldots,2.0$) and arranging the values into a vector. 

The parameter `k` in `computePL()` is the order of the persistence landscape function. The `p` in `computePS()` is the power of the weights for the silhouette function. 

To compute the summaries for homological dimension $H_1$, set the `homDim` argument equal to 1:

```{r}
# compute the PL and PS summaries for homological dimension H_1
computePL(D,homDim = 1,scaleSeq,k=1)
computePS(D,homDim = 1,scaleSeq,p=1)
```

The `TDA` package also provides implementations of the persistence landscapes and silhouettes. Below we compare the two implementations in terms of accuracy of results and run-time cost. 

```{r}
pl1 <- computePL(D,homDim = 0,k=1,scaleSeq)
pl2 <- as.vector(landscape(D,dimension = 0,KK = 1, tseq = scaleSeq))
all.equal(pl1,pl2) # -> TRUE (the results are the same)

compCost <- microbenchmark(
  computePL(D,homDim = 0,k=1,scaleSeq),
  landscape(D,dimension = 0,KK = 1, tseq = scaleSeq),
  times = 500
)
sm <- summary(compCost)
costRatioPL <- sm$mean[2]/sm$mean[1] # ratio of computational time means
print(costRatioPL)
```

For homological dimension $H_0$, it took `TDA::landscape()` about `r round(costRatioPL)` times more time than `TDAvec::computePL()`. Similarly, we can compare `TDA::silhouette()` and `TDAvec::computePL()`:

```{r}
ps1 <- computePS(D,homDim = 0, p = 1,scaleSeq)
ps2 <- as.vector(silhouette(D,dimension = 0,p = 1, tseq = scaleSeq))
all.equal(ps1,ps2) # -> TRUE (the results are the same)

compCost <- microbenchmark(
  computePS(D,homDim = 0, p = 1,scaleSeq),
  silhouette(D,dimension = 0,p = 1, tseq = scaleSeq),
  times = 500
)
sm <- summary(compCost)
costRatioPS <- sm$mean[2]/sm$mean[1]
print(costRatioPS)
```

To discretize the _persistent entropy summary function_, _Euler characteristic curve_ and _Betti curve_, we employ a different vectorization scheme. Rather than evaluating a summary function at increasing scales and arranging the values into a vector, we compute the average values of the summary function between two
consecutive scale points using integration. More specifically, if $f$ is a (univariate) summary function and $t_1,t_2,\ldots,t_n$ are increasing scale values, we discretize $f$ as:

\begin{equation}
\Big(\frac{1}{\Delta t_1}\int_{t_1}^{t_2}f(t)dt,\frac{1}{\Delta t_2}\int_{t_2}^{t_3}f(t)dt,\ldots,\frac{1}{\Delta t_{n-1}}\int_{t_{n-1}}^{t_n}f(t)dt\Big)\in\mathbb{R}^{n-1},
\end{equation}

where $\Delta t_k=t_{k+1}-t_k$, $k=1,\ldots,n-1$. For the above three summary functions, the computation of integrals can be done analytically and efficiently implemented. For the Betti curve, we use the identity weight function (i.e., $w(b,d)\equiv 1$). If the weight function $w(b,d)=-(l/L)\log_2(l/L)$, where $l=d-b$ (life-span) and $L=\sum_{i=1}^n l_i$ (sum of all life-spans), the vector summary of the Betti curve will be the same as that of the persistent entropy summary function.

```{r}
# Persistent Entropy Summary (PES) function
# compute PES for homological dimension H0
computePES(D,homDim = 0,scaleSeq) 
# compute PES for homological dimension H1
computePES(D,homDim = 1,scaleSeq)

# Euler Characteristic Curve (ECC) 
computeECC(D,maxhomDim = 1,scaleSeq) # maxhomDim = maximal homological dimension considered

# Vector of Averaged Bettis (VAB) - a vectorization of Betti Curve
# compute VAB for homological dimension H0
computeVAB(D,homDim = 0,scaleSeq) 
# compute VAB for homological dimension H1
computeVAB(D,homDim = 1,scaleSeq)
```

To discretize the _persistence surface_ and _persistence block_, we first need to switch from the birth-death to the birth-persistence coordinates. 

```{r}
D[,3] <- D[,3] - D[,2] 
colnames(D)[3] <- "Persistence"
```

The resulting vector summaries are called the _persistence image_ (PI) and the _vectorized of persistence block_ (VPB) respectively. 

```{r}
# Persistence Image (PI)
res <- 5 # resolution or grid size
# find min and max persistence values
minPH0 <- min(D[D[,1]==0,3]); maxPH0 <- max(D[D[,1]==0,3])
sigma <- 0.5*(maxPH0-minPH0)/res # default way of selecting the standard deviation sigma of the Gaussians on top of each point of the diagram
# construct one-dimensional grid of scale values
ySeqH0 <- seq(minPH0,maxPH0,length.out=res+1)
# compute PI for homological dimension H_0
computePI(D,homDim=0,xSeq=NA,ySeqH0,res,sigma)

# Vectorized Persistence Block (VPB)
# construct one-dimensional grid of scale values
ySeqH0 <- unique(quantile(D[D[,1]==0,3],probs = seq(0,1,by=0.2))) 
tau <- 0.3 # parameter in [0,1] which controls the size of blocks around each point of the diagram 
# compute VPB for homological dimension H_0
computeVPB(D,homDim = 0,xSeq=NA,ySeqH0,tau) 
```

Since the $H_0$ features all have the birth value of zero in this case, a one-dimensional grid of scale values is used for vectorization.  

For homological dimension $H_1$, the birth values are not all the same and therefore the vectorization is performed over a two-dimensional grid. For the VPB summary, since the blocks around each point of the persistence diagram have different sizes, we construct the grid with scale values spread out non-uniformly (i.e. the rectangular grid cells have different dimensions). In experiments, this construction of the grid tends to provide better performance over the grid with equal cell sizes. 

```{r}
# PI
res <- 5 # resolution or grid size
# find min & max birth and persistence values
minBH1 <- min(D[D[,1]==1,2]); maxBH1 <- max(D[D[,1]==1,2])
minPH1 <- min(D[D[,1]==1,3]); maxPH1 <- max(D[D[,1]==1,3])
xSeqH1 <- seq(minBH1,maxBH1,length.out=res+1)
ySeqH1 <- seq(minPH1,maxPH1,length.out=res+1)
sigma <- 0.5*(maxPH1-minPH1)/res
# compute PI for homological dimension H_1
computePI(D,homDim=1,xSeqH1,ySeqH1,res,sigma)

# VPB
xSeqH1 <- unique(quantile(D[D[,1]==1,2],probs = seq(0,1,by=0.2)))
ySeqH1 <- unique(quantile(D[D[,1]==1,3],probs = seq(0,1,by=0.2)))
tau <- 0.3
# compute VPB for homological dimension H_1
computeVPB(D,homDim = 1,xSeqH1,ySeqH1,tau) 
```

As a note, the code for `computePI()` is adopted from the `pers.image()` function (available in the `kernelTDA` package) with minor modifications. For example, `pers.image()` uses a one-dimensional grid for homological dimension $H_0$ regardless of the filtration type. In contast, `computePI()` uses a one-dimensional grid only if additionally the birth values are the same (which may not be true for some filtrations such as the lower-star filtration). 


#### References

1. Bubenik, P. (2015). Statistical topological data analysis using persistence landscapes. _Journal of Machine Learning Research_, 16(1), 77-102. 

2. Chazal, F., Fasy, B. T., Lecci, F., Rinaldo, A., & Wasserman, L. (2014, June). Stochastic convergence of persistence landscapes and silhouettes. In _Proceedings of the thirtieth annual symposium on Computational geometry_ (pp. 474-483).

3. Atienza, N., Gonzalez-Díaz, R., & Soriano-Trigueros, M. (2020). On the stability of persistent entropy and new summary functions for topological data analysis. _Pattern Recognition_, 107, 107509.

4. Richardson, E., & Werman, M. (2014). Efficient classification using the Euler characteristic. _Pattern Recognition Letters_, 49, 99-106.

5. Chazal, F., & Michel, B. (2021). An Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists. _Frontiers in Artificial Intelligence_, 108.

6. Chung, Y. M., & Lawson, A. (2022). Persistence curves: A canonical framework for summarizing persistence diagrams. _Advances in Computational Mathematics_, 48(1), 1-42.

7. Adams, H., Emerson, T., Kirby, M., Neville, R., Peterson, C., Shipman, P., ... & Ziegelmeier, L. (2017). Persistence images: A stable vector representation of persistent homology. _Journal of Machine Learning Research_, 18.

8. Chan, K. C., Islambekov, U., Luchinsky, A., & Sanders, R. (2022). A computationally efficient framework for vector representation of persistence diagrams. _Journal of Machine Learning Research_, 23, 1-33.

